#### request封装请求头
- from urllib.request import Request,urlopen
- url='http://baidu.com'
- headers ={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36'}
- request = Request(url,headers=headers)
- response = urlopen(request)
- response_data = response.read()
- print(response_data.decode("utf-8"))

####  Get方式--汉字urlencode编码问题
- url中的汉字需要转成urlencode编码
- from urllib import parse
- wd = {"wd":"尚硅谷"}
- urlencode = parse.urlencode(wd)
- 解码:decode = parse.unquote("wd=%E5%B0%9A%E7%A1%85%E8%B0%B7")

#### POST请求
- 1.data ={"name":"zhangsan","age":"18"}字典
- 2.data= urlencode(data).encode("utf-8")
- 3.request = urllib.request(url,data=data)

#### 代码访问带有CA认证的网站
- 1.import ssl
- 2.context = ssl._create_unverified_context()
- 3.response = urlopen(request,context=context)

#### urllib.request：Handler处理器和自定义Opener
**自定义opener**
- from urllib.request import HTTPHandler,Request,build_opener
- 1.http_handler=HTTPHandler() 创建HTTPHandler实例对象
- 2.opener=build_opener(http_handler)创建支持处理HTTP请求的opener对象
- 3.url='www.baidu.com'
- 4.request=Request(url)
- 5.response=opener.open(request)

**ProxyHandler**
- proxy_hander = ProxyHandler({"http":"114.67.228.126:16819"})
- opener = build_opener(proxy_hander)

**获取Cookie，并保存到CookieJar()对象中**
- from urllib.request import HTTPCookieProcessor,build_opener
- from http.cookiejar import CookieJar
- 1.cookiejar = CookieJar()构建一个cookiejar对象实例存储cookiejar
- 2.handler = HTTPCookieProcessor(cookiejar=cookiejar) # 使用HTTPCookieProcessor()来创建cookie处理器对象
- 3.opener = build_opener(handler)
- 4.response = opener.open("http://www.baidu.com/")
- 5.打印出cookie:
>
    cookie_str = ""
    for item in cookiejar:
       # print(item)
       cookie_str = cookie_str +item.name+"="+item.value+";"
    cookie_str = cookie_str[:-1]#把最后一个分号干掉
    print(cookie_str)


**利用cookie登录**
- 利用浏览器登录抓取cookie
- 使用sublime处理handler:
>     - 替换:正则---findwhat: ^(.*): (.*)$ Replacewhat: "\1":"\2",
- from urllib.request import Request,urlopen
- headers={填入处理好的header信息}
- request=Request(url,headers=headers)
- response=urlopen(request)


****
#### Requests
>
    import request
    response = requests.get("http://www.baidu.com/",params=,headers=)
    print(response.request) #打印出是什么类型的请求
    print(response.content) #打印出返回的二进制内容
    print(response.text) #打印解码后的数据

**保存图片**
>
    with open('name.jpg','wb') as f:
        for block in response.iter_content(1024):
            if not block:
                break
            f.write(response.content)
****
#### 正则
- match
>
    content='hello world python'
    pattern=re.compile(r'python')
    print(pattern.match(content)

- search/findall
>   
    pattern=re.compile(r'\d')
    content='qwer1234'
    print(pattern.search(content)
    得到一个,findall得到全部
- re.S和re.I
    - pattern=re.compile(r'\d',re.S)
    - re.L忽略大小写
    - re.S不使用则在每一行内匹配,而使用re.S后会将字符串作为一个整体,将\n当做一个普通字符
----

**爬取内涵段子**
>
    #需求,爬取内涵吧http://www.neihan8.com/article/list_5_1.html的段子
    import requests
    import re
    class Spider(object):
    	#第一步,请求页面的数据
    	#page就是爬取的页数
    	def loader_page(self,page):
    		url = "http://www.neihan8.com/article/list_5_"+str(page)+".html"
    		response = requests.get(url)
    		return response.content
    	def write_file(self,item):
    		with open("内涵吧段子.txt","a") as f:
    			f.write(item)
    if __name__ == "__main__":
    	print("""
    		内涵吧小爬虫开始干活了
    	""")
    	page = 1
    	spider = Spider()
    	swicth = True
    	while swicth:
    		cmd = input("请按回车键,爬虫开干,输入quit退出爬取:")
    		if cmd == "quit":
    			swicth = False
    		print("当前正在爬取[%d]页面" % page)
    		content = spider.loader_page(page)
    		content = content.decode("gbk")
    		# print(content)
    		#第二步,使用正则得到段子数据,数据清洗
    		pattern = re.compile(r'<div class="f18 mb20">(.*?)</div>',re.S)
    		lists = pattern.findall(content)
    		for item in lists:
    			item = item.replace("<p>","").replace("</p>","").replace("<br />","").replace("&ldquo;","").replace("&hellip;","").replace("&rdquo;","")
    			print(item)
    			spider.write_file(item)
    		page += 1

****

**XML和HTML**
- XML:可扩展标记语言,被设计为传输和存储数据，其焦点是数据的内容。
- HTML:超文本标记语言,显示数据以及如何更好显示数据。

- HTML DOM:	Document Object Model for HTML (文档对象模型)	通过 HTML DOM，可以访问所有的 HTML 元素，连同它们所包含的文本和属性。可以对其中的内容进行修改和删除，同时也可以创建新的元素。

**XML节点**
- 父 (Parent) :每个元素以及属性都有一个父。
- 子 (Children) :元素节点可有零个、一个或多个子。
- 同胞（Sibling）:拥有相同的父的节点
- 先辈（Ancestor）:某节点的父、父的父，等等

**XPath定义**
XPath (XML Path Language) 是一门在 XML 文档中查找信息的语言，可用来在 XML 文档中对元素和属性进行遍历。

**XPath语法**
- nodename	选取此节点的所有子节点。
- /	从根节点选取。
- //	从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。(从任何地方开始不用使用/根节点一级一级往下找)
- .	选取当前节点。
- ..	选取当前节点的父节点。
- @	选取属性。要加上中括号
- //@name 选取所有为name的属性
- 选取未知节点:
>     - *	匹配任何元素节点。
    - @*匹配任何属性节点。需要结合其他使用。
    - node()匹配任何类型的节点。
- 运算符
    - 选择多个使用 | 连接 例.11
    - 加+ 减- 乘* 除div---->返回计算结果
    - 等于= 不等于!= 小于< 小于等于<= 或or 与and --->返回值是true和false
    - mod 计算出发的余数
- 得到文本信息: //name/text()
- 实例应用:
>
    1. /bookstore/book[1]	选取属于 bookstore 子元素的第一个 book 元素。
    2. /bookstore/book[last()]	选取属于 bookstore 子元素的最后一个 book 元素。
    3. /bookstore/book[last()-1]	选取属于 bookstore 子元素的倒数第二个 book 元素。
    4. /bookstore/book[position()<3]	选取最前面的两个属于 bookstore 元素的子元素的 book 元素。
    5. //title[@lang]	选取所有拥有名为 lang 的属性的 title 元素。
    6. /bookstore/book[price>35.00]	选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。
    7. /bookstore/book[price>35.00]/title	选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。
    8. /bookstore/*	选取 bookstore 元素的所有子元素。
    9. //*	选取文档中的所有元素。
    10. //title[@*]	选取所有带有属性的 title 元素。
    11. //book/title | //book/price	选取 book 元素的所有 title 和 price 元素。

---

#### Python内使用XPath

- 使用 **lxml** 的 **etree** 库进行转码
    - html = etree.HTML(text) 使用etree.HTML()可以将字符串转为HTML文档(lxml 可以自动修正 html 代码)
    - result  = etree.tostring(html) 使用etree.tostring()可以将html文档转换为字符串
    - html = etree.parse('./hello.html') etree.parse可以读取本地文件
- result = html.xpath('//li') 使用xpath()函数提取目标,括号里面是XPath语法

**爬取百度贴吧的图片**
>
    from urllib import parse
    from urllib.request import Request,urlopen
    from lxml import etree
    import os

    num = 1

    def save(link):
    	global num

    	request = Request(link)
    	response = urlopen(request)
    	data = response.read()

    	if not os.path.exists("./image/"):
    		os.makedirs("./image/")


    	with open("./image/第"+str(num)+"张图片.jpg","wb") as f:
    		f.write(data)
    		print("第"+str(num)+"张图片保存完毕")
    	num += 1

    #url是帖子的链接
    def get_tiezi_image_url(url):
    	# headers ={
    	# 	"User-Agent": "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/7.0)"
    	# }
    	#得到图片的图片的链接
    	request = Request(url)
    	response = urlopen(request)
    	data = response.read()

    	#得到图片的xpath代码
    	content = data.decode()
    	# print(content)
    	html = etree.HTML(content)
    	list_links = html.xpath('//div[@class="d_post_content j_d_post_content "]/img/@src')
    	# print(list_links)
    	for link in list_links:
    		print("图片链接===",link)
    		save(link)


    #得到贴吧帖子的链接
    def get_tieba_link(data):
    	content  = data.decode()
    	# print(content)
    	html = etree.HTML(content)
    	list_links = html.xpath('//div[@class="threadlist_title pull_left j_th_tit "]//a/@href')
    	# print(list_links)
    	for link in list_links:
    		url = "https://tieba.baidu.com"+link
    		# print(url)
    		get_tiezi_image_url(url)

    def baidu_download(newurl,num):
    	request=Request(newurl)
    	response=urlopen(request)
    	data=response.read()
    	# print(data)
    	# save(data,num)
    	get_tieba_link(data)

    def baidu_spider(url,startpage,endpage):
    	for num in range(startpage,endpage+1):
    		pn = (num-1)*50
    		newurl = url + "&ie=utf-8&pn=" + str(pn)
    		baidu_download(newurl,num)

    def main():
    	kw = input("请输入要爬取的贴吧的名称:")
    	startpage = int(input("请输入起始页面"))
    	endpage = int(input("请输入结束页面"))
    	kw = {"kw":kw}
    	kw = parse.urlencode(kw)

    	url = "https://tieba.baidu.com/f?"+kw

    	baidu_spider(url,startpage,endpage)

    if __name__ == '__main__':
    	main()
